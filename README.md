# Interpretability-of-Deep-Learning-Models
Dual degree project thesis on "Interpretability of Deep Learning Models" (July 2019 - May 2020). Work done under the guidance of Dr. Mitesh M. Khapra at IIT Madras.

## Abstract

The past few years have seen an explosion in the development of artificial intelligence systems to tackle a plethora of human tasks, starting with simple tasks such as image classification, and going on to more complex ones such as answering questions based on reading passages or images, language translation, or playing games such as Alpha-Go or Minecraft. With the increasing size and complexity of deep learning models, accuracy on various tasks has increased tremendously (almost human-level); it is now necessary to take a step back and analyze whether these models are working in an explainable and interpretable way. This dual degree project consists of two concurrent threads, with ***Interpretability of Deep Learning Systems*** as the central theme: **Analyzing Interpretability of Deep RCQA Systems**, and **Dialog-Based Image Retrieval**.

## List of publications based on thesis

* [Towards Interpreting BERT for Reading Comprehension Based QA](https://aclanthology.org/2020.emnlp-main.261) (short paper accepted at **EMNLP 2020**)
* [A Framework for Rationale Extraction for Deep QA models](https://arxiv.org/abs/2110.04620) (**arXiv preprint**, uploaded 2021)
* [Scene Graph based Image Retrieval--A case study on the CLEVR Dataset](https://arxiv.org/abs/1911.00850) (extended abstract awarded the **Best Paper** at **LINGIR Workshop, ICCV 2019**).

